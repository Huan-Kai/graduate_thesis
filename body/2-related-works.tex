\hspace{24pt}
% \renewcommand{\baselinestretch}{1.5}

The main technique in this paper is to enhance the ability of cell line-specific cases in deep-learning based approach by introducing the features extracted from DNA methylation. Hence, we mainly review studies that use deep-learning based method, and integrate DNA sequences and various biological experimental data or not.

Over the past decade, there are many studies of data imputation for epigenetic data that have applied on deep-learning based methods. Zhou and Troyanskaya designed a model to observe the effect of non-coding variants, called DeepSEA. Although the main goal of this paper is not data imputation for histone modifications, they utilize CNN to extract regulatory sequence code from genomic sequence by learning to simultaneously predict large-scale chromatin-profiling data, including transcription factor, chromatin accessibility and histone modification, and use this model to analyze \cite{zhou2015predicting}. Quang and Xue proposed new hybrid framework of model, called DanQ, to solve same task with DeepSEA and improve predictions. In DanQ model, the convolution layers extract short-term information (regulatory motifs) from DNA sequences while the recurrent layers capture long-term dependencies between the regulatory motifs \cite{quang2016danq}. These methods select suitable architecture of model and take advantage of multi-task joint learning of various chromatin factors that share predictive features, and get satisfactory performance. But, these methods only focus on genomic features so that limit prediction of cell line-specific cases, because DNA sequences in any cell line are the same.

In order to overcome this limitation, these following methods introduce various biological experimental data, which is extracted by model to learn more cell line-specific information. Yin et al. designed CNNs to separately extract features from sequence information and chromatin accessibility data, and use joint module to combine features generated by CNNs. This model, called DeepHistone, not only accurate prediction of the binding site of histone modifications, but learn correct motifs in each cell line by different convolutional layers \cite{yin2019deephistone}. Baisya and Lonardi developed the model, called DeepPTM, to predict the binding sites of histone modifications from DNA sequences and transcription factor binding data, combined with a effective pre-processing step that they employ neighborhood cleaning rule to remove noisy training data. And, they prove that more accurate prediction from transcription factor binding data than from chromatin accessibility data \cite{baisya2020prediction}. These papers both select the biological experimental data highly related to gene expression. Lanchabtin and Qi further introduced 3D structure data of DNA interaction for better representation of DNA, due to 3D organization of chromatin in the cell have effects on regulatory regions, like promoter and enhancer. The model, called ChromeGCN, first extracts local genomic features from DNA sequences by convolutional layers, and then profile long-range 3D genomic information from organization of chromatin by graph convolutional layers \cite{lanchantin2020graph}.Together, these studies indicate that deep-learning based method is able to extract patterns from biological domain and indeed improve performance by introducing other biological experimental data.

However, from view of data, above biological experimental data introduced is not general enough. Chromatin accessibility introduced by DeepHistone might not account for the entire genome \cite{yan2016genome}. Although this situation is not apparent in current advanced measuring technologies, chromatin accessibility data actually has lower coverage over whole genome than DNA methylation data, shown in Table\ref{t2}. 3D organization of chromatin, Hi-C data, introduced by ChromeGCN have relatively low resolution in most available datasets, such as 25 kb or 40 kb \cite{zhang2018enhancing}. It brings not much information of high resolution for histone modification prediction. There are various kinds of transcription factors which is introduced by DeepPTM. But, many transcription factors data of most cell lines are not available on ENCODE website. As a result, DeepPTM is individually trained on same deep-learning framework against different cell lines, and generate models corresponding to cell lines. For instance, one is that using 30 transcription factors of H1, a kind of cell line, to predict 4 histone modifications, and another is that using 45 transcription factors of K562 to predict 3 histone modifications.

\begin{table}[H]%加入table環境指令以控制表格的位置、編號與標題，[h]代表將表格置於here，其他位置的標示請參考手冊
    \centering
    \begin{tabular}{lcc}
        \hline
        Cell line &  DNA methylation & Chromatin accessibility \\\hline
        A549 & 43582049 & 27235575 \\
        GM12878 & 38461546 & 10309402 \\
        K562 & 26944982 & 21450010 \\\hline
    \end{tabular}
    \renewcommand{\baselinestretch}{1.0}
    \captionsetup{labelfont=bf}
    \caption[Amount of active signals of coverage in whole genome]{Amount of active signals of coverage in whole genome. These cell lines are selected in this study to analyze}
    \label{t2}
\end{table}

From view of model, although DeepPTM has solved the problem of data generalization, it loses the advantages of multi-task model, such as data efficiency, reduced overfitting through shared representations, and fast learning by leveraging auxiliary information \cite{crawshaw2020multi}. Additionally, in most of mentioned methods implemented by CNN, the kernel size is fixed in each convolutional layer. In DeepSEA, kernel size of all convolution layers is set as eight \cite{zhou2015predicting}. In DnaQ, kernel size of a convolutional layer on top of RNN is set as twenty-six \cite{quang2016danq}. In DeepHistone, kernel size of all convolutional layers is set as nine \cite{yin2019deephistone}. In ChromeGCN, kernel size of all convolutional layers on top of graph convolutional layer is set as eight \cite{lanchantin2020graph}. Nevertheless, recent researches on CNN indicate that extracting features from the same input with different kernel sizes, called “inception module” by Szegedy et al., make model get better performance \cite{szegedy2015going}\cite{tan2019mixconv}. This framework is not only good for image data but sequential data, including DNA sequences. Published bioinformatics methods have been shown apparent improvement by successfully introducing inception module into model. For instance, Zhang et al. designed a inception-like network with gating mechanism to concurrently capturing multiple local patterns and long-term association in DNA sequences, to predict nucleosome position \cite{zhang2018lenup}.