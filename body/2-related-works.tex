\hspace{24pt}
% \renewcommand{\baselinestretch}{1.5}

In this section we review previous studies that employ deep-learning based methods and integrate DNA sequences and various biological experimental data to predict histone modification.

Over the past decade, there are many studies of data imputation for epigenetic data that have applied deep-learning based methods. Zhou and Troyanskaya designed a model to observe the effect of non-coding variants, called DeepSEA. Although the main goal of their paper is not data imputation for histone modifications, they utilize CNN to extract regulatory sequence code from genomic sequence by learning to simultaneously predict large-scale chromatin-profiling data, including transcription factor, chromatin accessibility and histone modification, and use this model to analyze \cite{zhou2015predicting}. Quang and Xue proposed a new hybrid framework model, called DanQ, to solve the same task as DeepSEA and improve predictions. In the DanQ model, the convolution layers extract short-term information (regulatory motifs) from DNA sequences while the recurrent layers capture long-term dependencies between the regulatory motifs \cite{quang2016danq}. These methods select suitable architecture of model and take advantage of multi-task joint learning of various chromatin factors that share predictive features, and obtain satisfactory performance. But, these methods only focus on genomic features so that limits prediction of cell line-specific cases, because DNA sequences in any cell line are the same.

In order to overcome this limitation, these following methods introduce various biological experimental data, which is extracted by models to learn more cell line-specific information. Yin et al.\ designed CNNs to separately extract features from sequence information and chromatin accessibility data, and use joint module to combine features generated by CNNs. This model, called DeepHistone, not only accurate predicts the binding site of histone modifications, but learns correct motifs in each cell line by different convolutional layers \cite{yin2019deephistone}. Baisya and Lonardi developed a model, called DeepPTM, to predict the binding sites of histone modifications from DNA sequences and transcription factor binding data, combined with an effective pre-processing step in which they employ neighborhood cleaning rule to remove noisy training data. And, they report more accurate prediction from transcription factor binding data than from chromatin accessibility data \cite{baisya2020prediction}. These papers both select the biological experimental data highly related to gene expression. Lanchabtin and Qi further introduced 3D structure data of DNA interaction for better representation of DNA, due to 3D organization of chromatin in the cell have effects on regulatory regions, like promoter and enhancer. The model, called ChromeGCN, first extracts local genomic features from DNA sequences by convolutional layers, and then profile long-range 3D genomic information from organization of chromatin by graph convolutional layers \cite{lanchantin2020graph}.  Together, these studies indicate that deep-learning based method is able to extract patterns from biological domain and indeed improve performance by introducing other biological experimental data.

However, from view of data, above biological experimental data introduced is not general enough. Chromatin accessibility introduced by DeepHistone might not account for the entire genome \cite{yan2016genome}. Although this situation is not apparent in current advanced measurement technologies, chromatin accessibility data actually has lower coverage over whole genome than DNA methylation data, shown in Table~\ref{t2}. 3D organization of chromatin, Hi-C data, introduced by ChromeGCN have relatively low resolution in most available datasets, such as 25kb or 40kb \cite{zhang2018enhancing}.  This resolution is insufficient for histone modification prediction. There are various kinds of transcription factors which are considered by DeepPTM. But, many for many transcription factors binding data in most cell lines are not available on ENCODE website. As a result, DeepPTM is individually trained on same deep-learning framework against different cell lines, and generates models corresponding to cell lines. For instance, one is that using 30 transcription factors of H1, a kind of cell line, to predict 4 histone modifications, and another is that using 45 transcription factors of K562 to predict 3 histone modifications.

\begin{table}[H]%加入table環境指令以控制表格的位置、編號與標題，[h]代表將表格置於here，其他位置的標示請參考手冊
    \centering
    \begin{tabular}{lcc}
        \hline
        Cell line &  DNA methylation & Chromatin accessibility \\\hline
        A549 & 43582049 & 27235575 \\
        GM12878 & 38461546 & 10309402 \\
        K562 & 26944982 & 21450010 \\\hline
    \end{tabular}
    \renewcommand{\baselinestretch}{1.0}
    \captionsetup{labelfont=bf}
    \caption[Amount of active signals of coverage in whole genome]{Amount of active signals of coverage in whole genome. These cell lines are selected in this study to analyze}
    \label{t2}
\end{table}

In terms of model design, although DeepPTM has solved the problem of data generalization, it loses the advantages of multi-task model, such as data efficiency, reduced overfitting through shared representations, and fast learning by leveraging auxiliary information \cite{crawshaw2020multi}. Additionally, in most of the above mentioned methods implemented by CNN, the kernel size is fixed in each convolutional layer. In DeepSEA, kernel size of all convolution layers is set as eight \cite{zhou2015predicting}. In DnaQ, kernel size of a convolutional layer on top of RNN is set as twenty-six \cite{quang2016danq}. In DeepHistone, kernel size of all convolutional layers is set as nine \cite{yin2019deephistone}. In ChromeGCN, kernel size of all convolutional layers on top of graph convolutional layer is set as eight \cite{lanchantin2020graph}. Nevertheless, recent researches on CNN indicate that extracting features from the same input with different kernel sizes, called “inception module” by Szegedy et al., can improve performance \cite{szegedy2015going}\cite{tan2019mixconv}. This framework is not only good for image data but sequential data, including DNA sequences. Published bioinformatics methods have been shown apparent improvement by successfully introducing inception module into model. For instance, Zhang et al.\ designed a inception-like network with gating mechanism to concurrently capturing multiple local patterns and long-term association in DNA sequences, to predict nucleosome position \cite{zhang2018lenup}.
